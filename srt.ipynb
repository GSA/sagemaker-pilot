{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker session and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "s3_bucket = 'srt-sm'\n",
    "prefix = 'Scikit-LinearLearner-pipeline-srt'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Scikit estimator \n",
    "To run our Scikit-learn training script on SageMaker, we construct a `sagemaker.sklearn.estimator.sklearn` estimator, which accepts several constructor arguments:\n",
    "\n",
    " - **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    " - **source_dir**: Path (absolute or relative) to a directory with any other training source code dependencies aside from tne entry point file (default: None). Structure within this directory are preserved when training on Amazon SageMaker.\n",
    " - **role**: Role ARN\n",
    " - **train_instance_type (optional)**: The type of SageMaker instances for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    " - **sagemaker_session (optional)**: The session used to train on Sagemaker.\n",
    " - **output_path (optional)**: s3 location where you want the training result (model artifacts and optional output files) saved. If not specified, results are stored to a default bucket. If the bucket with the specific name does not exist, the estimator creates the bucket during the fit() method execution.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "entry_point = 'sklearn_featureizer.py'\n",
    "source_dir = 'pipeline'\n",
    "\n",
    "s3_ll_output_key_prefix = \"ll_training_output\"\n",
    "preprocessor_output_path = 's3://{}/{}/{}/{}'.format(s3_bucket, prefix, s3_ll_output_key_prefix, 'll_preprocessor')\n",
    "\n",
    "sklearn_preprocessor = SKLearn(source_dir = source_dir,\n",
    "                               entry_point = entry_point,\n",
    "                               role = role,\n",
    "                               train_instance_type = \"ml.c4.xlarge\",\n",
    "                               sagemaker_session = sagemaker_session,\n",
    "                               output_path = preprocessor_output_path)\n",
    "\n",
    "\n",
    "train_input = f's3://{s3_bucket}/{prefix}/srt_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_preprocessor.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform the training data \n",
    "With the preprocessor fitted, we can now preprocess our training data. We'll use **batch transform**, storing the output right back into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer_output_path = 's3://{}/{}/{}/{}'.format(s3_bucket, prefix, s3_ll_output_key_prefix, 'll_transformer')\n",
    "\n",
    "transformer = sklearn_preprocessor.transformer(instance_count = 1,\n",
    "                                               instance_type = 'ml.m4.xlarge',\n",
    "                                               assemble_with = 'Line',\n",
    "                                               accept =' text/csv',\n",
    "                                               output_path = transformer_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training input\n",
    "transformer.transform(train_input, content_type='text/csv')\n",
    "print(f'Waiting for transform job: {transformer.latest_transform_job.job_name}')\n",
    "transformer.wait()\n",
    "preprocessed_train = transformer.output_path\n",
    "print(f\"Transform job done. Outpath:  {preprocessed_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a LinearLearner\n",
    "Here we'll use our preprocessed training data to fit a LinearLearner Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "ll_image = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with a subset of a larger dataset that won't have the 100 dimensions from the chi-2 selection, we \n",
    "#need to manually fetch the dimensions in order to specify the feature_dim hyperparameter of the linear learner.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_out = f\"{preprocessed_train}/srt_train.csv.out\"\n",
    "train_df = pd.read_csv(train_out)\n",
    "# minus 1 to exclude the labels\n",
    "feature_dim = train_df.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_output_path = 's3://{}/{}/{}/{}'.format(s3_bucket, prefix, s3_ll_output_key_prefix, 'll_estimator')\n",
    "\n",
    "ll_estimator = sagemaker.estimator.Estimator(ll_image,\n",
    "                                             role, \n",
    "                                             train_instance_count = 1, \n",
    "                                             train_instance_type = 'ml.m4.2xlarge',\n",
    "                                             train_volume_size = 20,\n",
    "                                             train_max_run = 3600,\n",
    "                                             input_mode = 'File',\n",
    "                                             output_path = estimator_output_path,\n",
    "                                             sagemaker_session = sagemaker_session)\n",
    "\n",
    "ll_estimator.set_hyperparameters(feature_dim = feature_dim, \n",
    "                                 predictor_type = 'binary_classifier', \n",
    "                                 mini_batch_size = 5)\n",
    "\n",
    "ll_train_data = sagemaker.session.s3_input(preprocessed_train, \n",
    "                                           distribution = 'FullyReplicated',\n",
    "                                           content_type = 'text/csv', \n",
    "                                           s3_data_type = 'S3Prefix')\n",
    "\n",
    "data_channels = {'train': ll_train_data}\n",
    "\n",
    "ll_estimator.fit(inputs = data_channels, logs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Pipeline Model here. This sets up a sequence of models in a single endpoint; in this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "scikit_learn_inference_model = sklearn_preprocessor.create_model()\n",
    "linear_learner_model = ll_estimator.create_model()\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "\n",
    "sm_model = PipelineModel(name = model_name, \n",
    "                         role = role, \n",
    "                         models = [scikit_learn_inference_model, linear_learner_model])\n",
    "\n",
    "sm_model.deploy(initial_instance_count = 1, \n",
    "                instance_type = 'ml.c4.xlarge',\n",
    "                endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a request to the pipeline endpoint \n",
    "Here we'll use the deployed model to get predictions for our test data. \n",
    "\n",
    "Below, the `content_type` field configures the first container, while the `accept` field configures the last container.\n",
    "\n",
    "We make our request with the payload in `text/csv` format, since that is what our script currently supports. If other formats need to be supported, this would have to be added to the `output_fn()` method in our entrypoint. Note that we set the `accpet` to `application/json`, since LinearLearner does not support `text/csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "test_input = f's3://{s3_bucket}/{prefix}/srt_test.csv'\n",
    "\n",
    "test_df = pd.read_csv(test_input)\n",
    "\n",
    "def format_as_csv(text):\n",
    "    #since we make our request with the payload in text/csv format, we need to sanitize the text first\n",
    "    return text.replace(\",\",\"\").replace(\"\\n\",\"\")\n",
    "\n",
    "test_df['1'] = test_df['1'].apply(format_as_csv)\n",
    "x_test = \",\\n\".join(test_df['1'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "predictor = RealTimePredictor(endpoint = endpoint_name,\n",
    "                              sagemaker_session = sagemaker_session,\n",
    "                              serializer = csv_serializer,\n",
    "                              content_type = CONTENT_TYPE_CSV,\n",
    "                              accept = CONTENT_TYPE_JSON)\n",
    "\n",
    "predictions_str = predictor.predict(x_test)\n",
    "predictions_dict = json.loads(predictions_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df['0']\n",
    "y_pred = [i['predicted_label'] for i in predictions_dict['predictions']]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Endpoint \n",
    "Once we are finished with the endpoint, we clean up the resources since the endpoint incurs costs for as long as it is alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName = endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
