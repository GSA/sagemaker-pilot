{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker session and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "s3_bucket = 'srt-sm'\n",
    "prefix = 'Scikit-LinearLearner-pipeline-srt'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Scikit estimator \n",
    "To run our Scikit-learn training script on SageMaker, we construct a `sagemaker.sklearn.estimator.sklearn` estimator, which accepts several constructor arguments:\n",
    "\n",
    " - **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    " - **source_dir**: Path (absolute or relative) to a directory with any other training source code dependencies aside from tne entry point file (default: None). Structure within this directory are preserved when training on Amazon SageMaker.\n",
    " - **role**: Role ARN\n",
    " - **train_instance_type (optional)**: The type of SageMaker instances for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    " - **sagemaker_session (optional)**: The session used to train on Sagemaker.\n",
    " - **output_path (optional)**: s3 location where you want the training result (model artifacts and optional output files) saved. If not specified, results are stored to a default bucket. If the bucket with the specific name does not exist, the estimator creates the bucket during the fit() method execution.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "entry_point = 'sklearn_featureizer.py'\n",
    "source_dir = 'pipeline'\n",
    "\n",
    "s3_ll_output_key_prefix = \"ll_training_output\"\n",
    "model_output_path = 's3://{}/{}/{}/{}'.format(s3_bucket, prefix, s3_ll_output_key_prefix, 'll_preprocessor')\n",
    "\n",
    "grid_search = SKLearn(source_dir = source_dir,\n",
    "                      entry_point = entry_point,\n",
    "                      role = role,\n",
    "                      train_instance_type = \"ml.c4.xlarge\",\n",
    "                      sagemaker_session = sagemaker_session,\n",
    "                      output_path = model_output_path)\n",
    "\n",
    "train_input = f's3://{s3_bucket}/{prefix}/srt_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit({'train': train_input}, logs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Pipeline Model here. This sets up a sequence of models in a single endpoint; in this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "\n",
    "model = grid_search.create_model(role = role)\n",
    "\n",
    "model.deploy(initial_instance_count = 1, \n",
    "             instance_type = 'ml.c4.xlarge',\n",
    "             endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a request to the endpoint \n",
    "Here we'll use the deployed model to get predictions for our test data. \n",
    "\n",
    "We need to make our request with the payload in `text/csv` format, since that is what our script currently supports. If other formats need to be supported, this would have to be added to the `output_fn()` method in our entrypoint. Note that we set the `accept` to `application/json` to get our output that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "test_input = f's3://{s3_bucket}/{prefix}/srt_test.csv'\n",
    "\n",
    "test_df = pd.read_csv(test_input)\n",
    "\n",
    "def format_as_csv(text):\n",
    "    #since we make our request with the payload in text/csv format, we need to sanitize the text first\n",
    "    return text.replace(\",\",\"\").replace(\"\\n\",\"\")\n",
    "\n",
    "test_df['1'] = test_df['1'].apply(format_as_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "predictor = RealTimePredictor(endpoint = endpoint_name,\n",
    "                              sagemaker_session = sagemaker_session,\n",
    "                              serializer = csv_serializer,\n",
    "                              content_type = CONTENT_TYPE_CSV,\n",
    "                              accept = CONTENT_TYPE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request an inference for a single sample\n",
    "sample = test_df['1'].iloc[0]\n",
    "\n",
    "prediction_str = predictor.predict(sample)\n",
    "prediction_dict = json.loads(predictions_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request inferences for multiple samples\n",
    "samples = test_df['1'].tolist()\n",
    "predictions_str = predictor.predict(samples)\n",
    "predictions_dict = json.loads(predictions_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "\n",
    "y_true = test_df['0']\n",
    "y_pred = [i['predicted_label'] for i in predictions_dict['predictions']]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Endpoint \n",
    "Once we are finished with the endpoint, we clean up the resources since the endpoint incurs costs for as long as it is alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName = endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
