{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker session and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "s3_bucket = 'srt-sm'\n",
    "prefix = 'training'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Scikit estimator \n",
    "To run our Scikit-learn training script on SageMaker, we construct a `sagemaker.sklearn.estimator.sklearn` estimator, which accepts several constructor arguments:\n",
    "\n",
    " - **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "role: Role ARN\n",
    " - **train_instance_type (optional)**: The type of SageMaker instances for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    " - **sagemaker_session (optional)**: The session used to train on Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'linear_learner/sklearn_featurizer.py'\n",
    "\n",
    "sklearn_preprocessor = SKLearn(entry_point=script_path,\n",
    "                               role=role,\n",
    "                               train_instance_type=\"ml.c4.xlarge\",\n",
    "                               sagemaker_session=sagemaker_session)\n",
    "\n",
    "\n",
    "train_input = f's3://{s3_bucket}/{prefix}/srt_train.csv'\n",
    "print(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_preprocessor.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform the training data \n",
    "With the preprocessor fitted, we can now preprocess our training data. We'll use **batch transform**, storing the output right back into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sklearn_preprocessor.transformer(instance_count=1,\n",
    "                                               instance_type='ml.m4.xlarge',\n",
    "                                               assemble_with='Line',\n",
    "                                               accept='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training input\n",
    "transformer.transform(train_input, content_type='text/csv')\n",
    "print(f'Waiting for transform job: {transformer.latest_transform_job.job_name}')\n",
    "transformer.wait()\n",
    "preprocessed_train = transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a LinearLearner\n",
    "Here we'll use our preprocessed training data to fit a LinearLearner Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "ll_image = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_ll_output_key_prefix = \"ll_training_output\"\n",
    "s3_ll_output_location = 's3://{}/{}/{}/{}'.format(s3_bucket, prefix, s3_ll_output_key_prefix, 'll_model')\n",
    "\n",
    "ll_estimator = sagemaker.estimator.Estimator(ll_image,\n",
    "                                             role, \n",
    "                                             train_instance_count=1, \n",
    "                                             train_instance_type='ml.m4.2xlarge',\n",
    "                                             train_volume_size = 20,\n",
    "                                             train_max_run = 3600,\n",
    "                                             input_mode= 'File',\n",
    "                                             output_path=s3_ll_output_location,\n",
    "                                             sagemaker_session=sagemaker_session)\n",
    "\n",
    "ll_estimator.set_hyperparameters(feature_dim=10, predictor_type='regressor', mini_batch_size=32)\n",
    "\n",
    "ll_train_data = sagemaker.session.s3_input(preprocessed_train, \n",
    "                                           distribution='FullyReplicated',\n",
    "                                           content_type='text/csv', \n",
    "                                           s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': ll_train_data}\n",
    "\n",
    "ll_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Pipeline Model here. This sets up a list of models in a single endpoint; in this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "scikit_learn_inferencee_model = sklearn_preprocessor.create_model()\n",
    "linear_learner_model = ll_estimator.create_model()\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, \n",
    "                         role=role, \n",
    "                         models=[scikit_learn_inferencee_model, linear_learner_model])\n",
    "\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a request to the pipeline endpoint \n",
    "Here we'll use the deployed model to get a prediction for some dummy data. \n",
    "\n",
    "Below, the `content_type` field configures the first container, while the `accept` field configures the last container.\n",
    "\n",
    "We make our request with the payload in `text/csv` format, since that is what our script currently supports. If other formats need to be supported, this would have to be added to the `output_fn()` method in our entrypoint. Note that we set the `accpet` to `application/json`, since LinearLearner does not support `text/csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "payload = 'This document is not compliant!'\n",
    "y_true = 0\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name,\n",
    "                              sagemaker_session=sagemaker_session,\n",
    "                              serializer=csv_serializer,\n",
    "                              content_type=CONTENT_TYPE_CSV,\n",
    "                              accept=CONTENT_TYPE_JSON)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Endpoint \n",
    "Once we are finished with the endpoint, we clean up the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
