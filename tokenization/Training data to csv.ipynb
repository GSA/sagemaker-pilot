{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the labeled documents\n",
    "This notebook assumes:\n",
    " - You've got all of the labeled solicitaton documents within a directory named `labeled_fbo_docs`\n",
    " - You can use the awscli\n",
    " - You have already created an S3 bucket (our is named `srt-sm`).\n",
    "\n",
    "Below, we'll read in each document and extract the text along with the label (the label is in the file name). Although there are three lables (red, yellow and green), we're combining red and yellow as noncompliant ($0$) and treating green as compliant ($1$). This makes a binary classification challenge.\n",
    "\n",
    "\n",
    ">And since we're only piloting SageMaker, will reduce our total training dataset size down to just 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data = []\n",
    "for i, file in enumerate(os.listdir('labeled_fbo_docs')):\n",
    "    if i < 50:\n",
    "        if file.startswith('GREEN'):\n",
    "            target = 1\n",
    "        elif file.startswith('RED') or file.startswith('YELLOW'):\n",
    "            target = 0\n",
    "        else:\n",
    "            raise Exception(f\"A file isn't prepended with the target:  {file}\")\n",
    "\n",
    "        file_path = os.path.join(os.getcwd(), 'labeled_fbo_docs', file)\n",
    "        with open(file_path, 'r', errors = 'ignore') as f:\n",
    "            #do some newline replacing\n",
    "            text = f.read().replace(\"\\n\", ' ').strip()\n",
    "        data.append([target, text])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Training and Testing data\n",
    "Since our data is imbalanced, we'll use the `stratify` kwarg to split the data in a stratified fashion, using the labels array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.00% of the training data is a positive sample\n",
      "20.00% of the testing data is a positive sample\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = [i[0] for i in data]\n",
    "x = [i[1] for i in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "n_test_pos_samples = 100 * sum(y_test) / len(y_test)\n",
    "n_train_pos_samples = 100 * sum(y_train) / len(y_train)\n",
    "\n",
    "print(\"{:.2f}% of the training data is a positive sample\".format(n_train_pos_samples))\n",
    "\n",
    "print(\"{:.2f}% of the testing data is a positive sample\".format(n_test_pos_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the training data to csv\n",
    "Here we'll write the training and test data to two csvs, using pandas, which will be smart about escaping quotes and all the other weird characters in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                                                  1\n",
      "0  1  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...\n",
      "1  1  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...\n",
      "2  0  National Institutes of Health Clinical Center ...\n",
      "3  0  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...\n",
      "4  1  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...\n",
      "   0                                                  1\n",
      "0  0  This is a combined synopsis/solicitation for c...\n",
      "1  0  General Information:   Document Type:         ...\n",
      "2  1  Attachment 1                         Glossary ...\n",
      "3  0  STATEMENT OF WORK     (SOW)    FOR    317 RCS/...\n",
      "4  0  |SOLICITATION, OFFER AND AWARD                ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame([y_train, X_train]).transpose()\n",
    "print(train_df.head())\n",
    "test_df = pd.DataFrame([y_test, X_test]).transpose()\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('srt_train.csv', index = False)\n",
    "\n",
    "test_df.to_csv('srt_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push training data to S3\n",
    "You'll need to have installed the awscli prior to this step and have configured it to use the Key ID and Secret Access Key of your AWS account. You can do that with `aws configure` as documented [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to s3n://srt-sm/training/srt_train.csv\n",
      "Done writing to s3n://srt-sm/training/srt_test.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "region = boto3.Session().region_name\n",
    "bucket = 'srt-sm' \n",
    "prefix = 'training' # Used as part of the path in the bucket where we'll store the train and test data\n",
    "bucket_path = f'https://s3-{region}.amazonaws.com/{bucket}'\n",
    "\n",
    "for f in ['srt_train.csv', 'srt_test.csv']:\n",
    "    key = f'{prefix}/{f}'\n",
    "    s3.Bucket(bucket).Object(key).upload_file(f)\n",
    "    url = f's3n://{bucket}/{key}'\n",
    "    print(f'Done writing to {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenization",
   "language": "python",
   "name": "tokenization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
