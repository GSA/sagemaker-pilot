{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the labeled documents\n",
    "This notebook assumes:\n",
    " - You've got all of the labeled solicitaton documents within a directory named `labeled_fbo_docs`\n",
    " - You can use the awscli\n",
    " - You have already created an S3 bucket (our is named `srt-sm`).\n",
    "\n",
    "Below, we'll read in each document and extract the text along with the label (the label is in the file name). Although there are three lables (red, yellow and green), we're combining red and yellow as noncompliant ($0$) and treating green as compliant ($1$). This makes a binary classification challenge.\n",
    "\n",
    "\n",
    ">And since we're only piloting SageMaker, will reduce our total training dataset size down to just 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "data = []\n",
    "for i, file in enumerate(os.listdir('labeled_fbo_docs')):\n",
    "    if i < 50:\n",
    "        if file.startswith('GREEN'):\n",
    "            target = 1\n",
    "        elif file.startswith('RED') or file.startswith('YELLOW'):\n",
    "            target = 0\n",
    "        else:\n",
    "            raise Exception(f\"A file isn't prepended with the target:  {file}\")\n",
    "\n",
    "        file_path = os.path.join(os.getcwd(), 'labeled_fbo_docs', file)\n",
    "        with open(file_path, 'r', errors = 'ignore') as f:\n",
    "            #do some newline replacing\n",
    "            text = f.read().replace(\"\\n\", ' ').strip()\n",
    "        data.append([target, text])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Training and Testing data\n",
    "Since our data is imbalanced, we'll use the `stratify` kwarg to split the data in a stratified fashion, using the labels array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.00% of the training data is a positive sample\n",
      "20.00% of the testing data is a positive sample\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = [i[0] for i in data]\n",
    "x = [i[1] for i in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "n_test_pos_samples = 100 * sum(y_test) / len(y_test)\n",
    "n_train_pos_samples = 100 * sum(y_train) / len(y_train)\n",
    "\n",
    "print(\"{:.2f}% of the training data is a positive sample\".format(n_train_pos_samples))\n",
    "\n",
    "print(\"{:.2f}% of the testing data is a positive sample\".format(n_test_pos_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the training data to csv\n",
    "Here we'll write the training and test data to two csvs, using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame([y_train, X_train]).transpose()\n",
    "\n",
    "test_df = pd.DataFrame([y_test, X_test]).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker doesn't like input data with a single feature apparently. So we'll add zeros\n",
    "# see comment here: https://stackoverflow.com/questions/51635902/aws-sagemaker-unable-to-parse-csv\n",
    "train_df[2] = [0 for i in range(len(train_df))]\n",
    "test_df[2] = [0 for i in range(len(test_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('srt_train.csv', index = False)\n",
    "\n",
    "test_df.to_csv('srt_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll read in the files we wrote just to make sure we do it correctly in our sagemaker script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is a combined synopsis/solicitation for c...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>General Information:   Document Type:         ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Attachment 1                         Glossary ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>STATEMENT OF WORK     (SOW)    FOR    317 RCS/...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>|SOLICITATION, OFFER AND AWARD                ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  zero\n",
       "0     0.0  This is a combined synopsis/solicitation for c...   0.0\n",
       "1     0.0  General Information:   Document Type:         ...   0.0\n",
       "2     1.0  Attachment 1                         Glossary ...   0.0\n",
       "3     0.0  STATEMENT OF WORK     (SOW)    FOR    317 RCS/...   0.0\n",
       "4     0.0  |SOLICITATION, OFFER AND AWARD                ...   0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_df_check = pd.read_csv('srt_test.csv')\n",
    "test_df_check.columns = ['target', 'text', 'zero']\n",
    "test_df_check = test_df_check.astype({'target': np.float64, 'text': str, 'zero': np.float64})\n",
    "\n",
    "test_df_check.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push training data to S3\n",
    "You'll need to have installed the awscli prior to this step and have configured it to use the Key ID and Secret Access Key of your AWS account. You can do that with `aws configure` as documented [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to s3n://srt-sm/Scikit-LinearLearner-pipeline-srt/srt_train.csv\n",
      "Done writing to s3n://srt-sm/Scikit-LinearLearner-pipeline-srt/srt_test.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "region = boto3.Session().region_name\n",
    "bucket = 'srt-sm' \n",
    "prefix = 'Scikit-LinearLearner-pipeline-srt'\n",
    "bucket_path = f'https://s3-{region}.amazonaws.com/{bucket}'\n",
    "\n",
    "for f in ['srt_train.csv', 'srt_test.csv']:\n",
    "    key = f'{prefix}/{f}'\n",
    "    s3.Bucket(bucket).Object(key).upload_file(f)\n",
    "    url = f's3n://{bucket}/{key}'\n",
    "    print(f'Done writing to {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenization",
   "language": "python",
   "name": "tokenization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
