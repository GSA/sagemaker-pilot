{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/Output Interface for the BlazingText Algorithm\n",
    "For supervised training, the BlazingText training/validation files need to be in the RecordIO format. The files should contain a single record per line, starting with the label. Labels are words that are prefixed by the string __label__. Here is an example of a training/validation file:\n",
    "\n",
    "```\n",
    "\n",
    "__label__4  linux ready for prime time , intel says , despite all the linux hype , the open-source movement has yet to make a huge splash in the desktop market . that may be about to change , thanks to chipmaking giant intel corp .\n",
    "\n",
    "__label__2  bowled by the slower one again , kolkata , november 14 the past caught up with sourav ganguly as the indian skippers return to international cricket was short lived . \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charlessmcallister/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "print(\"Extracting text from labeled documents.\")\n",
    "data = []\n",
    "for file in os.listdir('labeled_fbo_docs'):\n",
    "    if file.startswith('GREEN'):\n",
    "        target = '__label__1'\n",
    "    elif file.startswith('RED') or file.startswith('YELLOW'):\n",
    "        target = '__label__0'\n",
    "    else:\n",
    "        raise Exception(f\"A file isn't prepended with the target:  {file}\")\n",
    "    \n",
    "    file_path = os.path.join(os.getcwd(), 'labeled_fbo_docs', file)\n",
    "    with open(file_path, 'r', errors = 'ignore') as f:\n",
    "        text = f.read().replace(\"\\n\", ' ').strip()\n",
    "    data.append((target, text))\n",
    "print(\"Done extracting text from labeled documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    #append the label\n",
    "    cur_row.append(row[0])\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    \n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, output_file):\n",
    "    pool = Pool(processes = multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, data)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter = ' ', lineterminator = '\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.72 s, sys: 2.06 s, total: 9.78 s\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preparing the training dataset\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "preprocess(train, 'srt.train')\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess(test, 'srt.validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing to S3\n",
    "You'll need to have installed the awscli prior to this step and have configured it to use the Key ID and Secret Access Key of your AWS account. You can do that with the `aws configure` command as documented [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to s3n://srt-sagemaker/training/srt.train\n",
      "Done writing to s3n://srt-sagemaker/training/srt.validation\n",
      "CPU times: user 4.42 s, sys: 4.23 s, total: 8.65 s\n",
      "Wall time: 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3 = boto3.resource('s3')\n",
    "region = boto3.Session().region_name\n",
    "bucket = 'srt-sagemaker' \n",
    "prefix = 'training' # Used as part of the path in the bucket where we'll store train and test data\n",
    "bucket_path = f'https://s3-{region}.amazonaws.com/{bucket}'\n",
    "\n",
    "data_to_upload = ['srt.train', 'srt.validation']\n",
    "for upload_file in data_to_upload:\n",
    "    key = f'{prefix}/{upload_file}'\n",
    "    s3.Bucket(bucket).Object(key).upload_file(upload_file)\n",
    "    url = f's3n://{bucket}/{key}'\n",
    "    print(f'Done writing to {url}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenization",
   "language": "python",
   "name": "tokenization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
